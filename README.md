# Distributed Retail Log Analytics Pipeline (Hadoop POC)

![Hadoop](https://img.shields.io/badge/Hadoop-3.x-yellow)
![Hive](https://img.shields.io/badge/Hive-Analytics-blue)
![MapReduce](https://img.shields.io/badge/MapReduce-Streaming-green)


## ğŸ“Œ Overview

This project demonstrates a production-inspired Big Data pipeline built
using the Hadoop ecosystem.\
It showcases how distributed storage, parallel processing, and SQL-based
analytics can be combined to analyze retail-style transaction data using
HDFS, MapReduce Streaming, and Hive.

Instead of using toy academic examples, this repository presents a
structured Proof-of-Concept (POC) that reflects real-world data
engineering workflow design.

------------------------------------------------------------------------

## ğŸ§± Architecture

    Local Dataset
          â”‚
          â–¼
    HDFS (Distributed Storage)
          â”‚
          â–¼
    MapReduce Streaming (Python)
          â”‚
          â–¼
    Hive External Table (SQL Analytics)

------------------------------------------------------------------------

## ğŸ›  Technologies Used

-   Hadoop Distributed File System (HDFS)
-   MapReduce Streaming (Python)
-   Apache Hive
-   Sqoop (Conceptual Ingestion Layer)
-   Ubuntu WSL + VS Code

------------------------------------------------------------------------

## ğŸ“Š Use Case

Retail-style transaction logs are ingested into HDFS as the raw data
layer.\
A custom MapReduce job aggregates transactions by day to simulate
business analytics reporting.\
Hive external tables are used to query distributed data using SQL
without duplication.

------------------------------------------------------------------------

## ğŸš€ Pipeline Flow

### 1ï¸âƒ£ Data Layer

Dataset stored inside the `/data` directory and uploaded into HDFS.

### 2ï¸âƒ£ Processing Layer

Python-based MapReduce scripts perform distributed aggregation.

### 3ï¸âƒ£ Analytics Layer

Hive external table enables SQL querying on HDFS data.

### 4ï¸âƒ£ Ingestion Design

Sqoop example demonstrates how relational databases could integrate into
this pipeline.

------------------------------------------------------------------------

## ğŸ“ Project Structure

    retail-log-hadoop-pipeline/
    â”‚
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ retail_logs.csv              # Raw dataset
    â”‚
    â”œâ”€â”€ mapreduce/
    â”‚   â”œâ”€â”€ mapper.py                    # MapReduce mapper logic
    â”‚   â””â”€â”€ reducer.py                   # MapReduce reducer logic
    â”‚
    â”œâ”€â”€ hive/
    â”‚   â””â”€â”€ analytics.sql                # Hive database & query definitions
    â”‚
    â”œâ”€â”€ ingestion/
    â”‚   â””â”€â”€ sqoop_example.txt            # Sqoop ingestion reference
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â””â”€â”€ run_pipeline.sh              # One-command pipeline execution
    â”‚
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ How to Run the Pipeline

``` bash
bash scripts/run_pipeline.sh
```

------------------------------------------------------------------------

## ğŸ“ˆ Example Output

    Fri   19
    Sat   87
    Sun   76
    Thur  61

------------------------------------------------------------------------

## ğŸ¯ Key Learning Outcomes

-   Distributed storage using HDFS
-   Parallel analytics using MapReduce Streaming
-   SQL-based querying with Hive external tables
-   Designing scalable ingestion architecture

------------------------------------------------------------------------

## ğŸ§  Engineering Notes

-   The column name `meal_time` was used instead of `time` to avoid
    conflicts with Hive reserved keywords.
-   External tables were used in Hive to prevent data duplication.

------------------------------------------------------------------------

## ğŸ”® Future Improvements

-   Integrate Apache Spark for faster analytics
-   Add ML feature engineering stage
-   Automate pipeline execution using CI/CD
